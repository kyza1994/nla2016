{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 11:  Great Iterative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture\n",
    "\n",
    "- Gaussian elimination and graphs in more details\n",
    "- Concept of **iterative methods** for linear systems\n",
    "- Richardson iteration and its convergence, Chebyshev acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "- Main iterative methods: conjugate gradient, GMRES, ...\n",
    "- Lanczos and Arnoldi orthogonalization of Krylov subspaces, optimality result for Krylov subspaces\n",
    "- Convergence estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution of linear systems and minimization of functionals\n",
    "\n",
    "Instead of solving a linear system, we can minimize the **residual:**\n",
    "\n",
    "$$R(x) = \\Vert A x - f \\Vert_2.$$\n",
    "\n",
    "The direct first-order conditions for the minimum of this functional gives\n",
    "\n",
    "$$A^* A x = A^* f,$$\n",
    "\n",
    "thus it has squared condition number, so direct minimization of the residual by standard optimization methods is rarely used.\n",
    "\n",
    "For the symmetric positive definite case there is a much simpler functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Energy functional\n",
    "\n",
    "Let $A = A^* > 0$, then the following functional \n",
    "\n",
    "$$\\Phi(x) = (Ax, x)  - 2(f, x)$$\n",
    "\n",
    "is strictly convex, and its global optimum satisfies\n",
    "\n",
    "$$A x_* = f.$$\n",
    "\n",
    "Indeed,\n",
    "\n",
    "$\n",
    "\\delta \\Phi = \\delta (Ax, x) - 2(f, \\delta x)=$  $(Ax, \\delta x) + (A \\delta x, x) - 2(f, \\delta x)$\n",
    "        $= ((A + A^{*})x, \\delta x) - 2(f, \\delta x) =$\n",
    "\n",
    "$\n",
    "= 2(A x - f, \\delta x) = 2 (\\nabla \\Phi, \\delta x)\n",
    "$.\n",
    "\n",
    "Thus, $$\\nabla \\Phi = 2(Ax - f).$$ (and simple iteration is the gradient descent) and the stationary point $\\nabla \\Phi = 0$ yields\n",
    "$$A x_* = f.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept of iterative methods\n",
    "\n",
    "We assume, that we can only multiply a matrix by vector as a black-box in a fast way (say, $\\mathcal{O}(N))$. <br>\n",
    "Nothing else!\n",
    "\n",
    "The most general situation: \n",
    "we compute \n",
    "\n",
    "$$y_k = A x_k, \\quad k = 1, \\ldots, M$$\n",
    "\n",
    "for some input vectors $x_k$, and then we have a linear subspace, generated by these $M$ vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximation of the solution by a subspace\n",
    "\n",
    "Given a linear $M$-dimensional subspace, we want to find an approximate solution of \n",
    "\n",
    "$$A x \\approx f, \\quad x = \\sum_{k=1}^M \\widehat x_k y_k,$$\n",
    "\n",
    "where $\\widehat x$ is the vector of coefficients.\n",
    "\n",
    "In the symmetric positive definite case  we need to minimize \n",
    "\n",
    "$$(Ax, x) - 2(f, x)$$ \n",
    "\n",
    "subject to $$x = Y \\widehat x,$$\n",
    "\n",
    "where $Y=[y_1,\\dots,y_M]$ is $n \\times M$ and vector $\\widehat x$ has length $M$.\n",
    "\n",
    "Using the representation of $x$, we have the following minimization for $\\widehat x$:\n",
    "\n",
    "$$\\widehat{\\Phi}(\\widehat x) = (A Y \\widehat x, Y \\widehat x) - 2(f, Y \\widehat x) = (Y^* A Y \\widehat x, \\widehat x) - 2(Y^* f, \\widehat x).$$\n",
    "\n",
    "Note that this is the same functional, but for the **Galerkin projection** of $A$\n",
    "\n",
    "$$Y^* A Y \\widehat x = Y^* f,$$\n",
    "\n",
    "which is an $M \\times M$  linear system with symmetric positive definite matrix if $Y$ has full column rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selection of the subspace\n",
    "\n",
    "Instead of multiplying different vectors $x_k$, in the Krylov subspace we generate the whole subspace from a single vector $f$:\n",
    "\n",
    "$$y_0\\equiv k_0 = f, \\quad y_1\\equiv k_1 = A f, \\quad y_2\\equiv k_2 = A^2 f, \\ldots, \\quad y_{M-1}\\equiv k_{M-1} = A^{M-1} f.$$\n",
    "\n",
    "This gives the **Krylov subpace**\n",
    "\n",
    "$$K_M(A, f) = \\mathrm{Span}(f, Af, \\ldots, A^{M-1} f).$$\n",
    "\n",
    "It is known to be quasi-optimal space given only matrix-vector product operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selection of the orthogonal basis in the subspace\n",
    "\n",
    "The natural basis in the Krylov subspace is very **ill-conditioned**, since \n",
    "\n",
    "$$k_i = A^i f \\rightarrow \\lambda_\\max^i v,$$\n",
    "\n",
    "where $v$ is the eigenvector, corresponding to the maximal eigenvalue of $A$,\n",
    "\n",
    "i.e. $k_i$ become more and more collinear.\n",
    "\n",
    "**Solution:** Compute orthogonal basis in the Krylov subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Good basis in a Krylov subspace\n",
    "\n",
    "In order to have stability, we first orthogonalize the vectors from the Krylov subspace using **Gram-Schmidt** orthogonalization process (or, QR-factorization).\n",
    "\n",
    "$$K_j = \\begin{bmatrix} f & Af & A^2 f & \\ldots & A^{j-1} f\\end{bmatrix} = Q_j R_j, $$\n",
    "\n",
    "and the solution will be approximated as $$x \\approx Q_j \\widehat{x}_j.$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Short way to Arnoldi relation\n",
    "\n",
    "The Krylov matrix $K_j$ satisfies an important recurrent relation (called **Arnoldi relation**)\n",
    "\n",
    "$$A Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1},$$\n",
    "\n",
    "where $H_j$ is upper Hessenberg, and  $Q_{j+1} = [q_0,\\dots,q_j]$.\n",
    "\n",
    "Let us prove it (given for $j = 3$ for simplicity):\n",
    "\n",
    "$$A \\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} = \\begin{bmatrix} k_1 & k_2 & k_3 \\end{bmatrix} = \n",
    "\\begin{bmatrix} k_0 & k_1 & k_2 \\end{bmatrix} \\begin{bmatrix} 0 & 0 & \\alpha_0 \\\\\n",
    "1 & 0  & \\alpha_1 \\\\\n",
    "0 & 1  & \\alpha_2 \\\\\n",
    "\\end{bmatrix} \n",
    " + \\begin{bmatrix} 0 & 0 & k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2\n",
    " \\end{bmatrix},\n",
    " $$\n",
    " where $\\alpha_s$ will be selected later. Denote $\\widehat{k}_3 = k_3  - \\alpha_0 k_0 - \\alpha_1 k_1 - \\alpha_2 k_2$.\n",
    " \n",
    " In the matrix form,\n",
    " \n",
    " $$A K_3 = K_3 Z + k_3 e^{\\top}_2,$$\n",
    " where $Z$ is the **lower shift** matrix with the last column $(\\alpha_0,\\alpha_1,\\alpha_2)^T$, and $e_2$ is the last column of the identity matrix.\n",
    " \n",
    " Let $$K_3 = Q_3 R_3$$ be the QR-factorization. Then,\n",
    " \n",
    " $$A Q_3 R_3 = Q_3 R_3 Z + \\widehat{k}_3 e^{\\top}_2,$$\n",
    " \n",
    " $$ A Q_3 = Q_3 R_3 Z R_3^{-1} + \\widehat{k}_3 e^{\\top}_2 R_3^{-1}.$$\n",
    "\n",
    "\n",
    "Note that \n",
    "\n",
    "$$e^{\\top}_2 R_3^{-1} = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} * & * & * \\\\\n",
    " 0 &  * & * \\\\\n",
    " 0 & 0 & * \n",
    " \\end{bmatrix}  = \\gamma e^{\\top}_2,$$\n",
    " \n",
    " and\n",
    " \n",
    " $$R_3 Z R_3^{-1} = \\begin{bmatrix} * & * & * \\\\* & * & * \\\\  0 & * & * \\\\ \\end{bmatrix}, $$            \n",
    "in the general case it will be an **upper Hessenberg matrix** $H$, i.e. a matrix \n",
    "that $$H_{ij} = 0, \\quad \\mbox{if } i > j + 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Almost) Arnoldi relation\n",
    "\n",
    "Let $Q_j$ be the orthogonal basis in the Krylov subspace, then we have almost the Arnoldi relation\n",
    "\n",
    "$$A Q_j = Q_j H_j +  \\widehat{k}_j e^{\\top}_{j-1},$$\n",
    "\n",
    "where $H_j$ is an upper Hessenberg matrix, and \n",
    "\n",
    "$$\\widehat{k}_j = k_j - \\sum_{s=0}^{j-1} \\alpha_j k_j.$$\n",
    "\n",
    "We select $\\alpha_j$ in such a way that \n",
    "\n",
    "$$Q^*_j \\widehat{k}_j = 0.$$\n",
    "\n",
    "Then, $\\widehat{k}_j = h_{j, j-1} q_j,$ where $q_j$ is the last column of $Q_{j+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arnoldi relation: final formula\n",
    "\n",
    "We have \n",
    "\n",
    "$$A Q_j = Q_j H_j + h_{j, j-1} q_j e^{\\top}_{j-1}.$$\n",
    "\n",
    "This is the crucial formula for the efficient generation of such subspaces.\n",
    "\n",
    "For non-symmetric case, it is just modified Gram-Schmidt.\n",
    "\n",
    "For the symmetric case, we have a much simpler form (Lanczos process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lanczos process\n",
    "\n",
    "If $A = A^*$, then\n",
    "\n",
    "$$Q^*_j A Q_j = H_j, $$\n",
    "\n",
    "thus $H_j$ is hermitian, and thus it is **tridiagonal**, $H_j = T_j$.\n",
    "\n",
    "This gives a short-term recurrence relation to generate the Arnoldi vectors $q_j$ without **full** orthogonalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lanczos process (2)\n",
    "\n",
    "$$ A Q_j = Q_j T_j + T_{j, j-1} q_j e^{\\top}_{j-1}.$$\n",
    "\n",
    "\n",
    "In order to get $q_j$, we need to compute just the last column of \n",
    "\n",
    "$$T_{j, j-1} q_j = (A Q_j - Q_j T_j) e_{j-1} = A q_{j-1} - T_{j-1, j-1} q_{j-1} - T_{j-2, j-1} q_{j-2}. $$\n",
    "\n",
    "The coefficients $\\alpha_j = T_{j-1, j-1}$ and $\\beta_j = T_{j-2, j-1}.$\n",
    "\n",
    "can be recovered from orthogonality constraints \n",
    "\n",
    "$(q_j, q_{j-1}) = 0, \\quad (q_j, q_{j-2}) = 0$\n",
    "\n",
    "**All the other constraints will be satisfied automatically!!**\n",
    "\n",
    "And we only need to store two vectors to get the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From direct Lanczos method to the conjugate gradient\n",
    "\n",
    "We can now get from the Lanczos recurrence to the famous **conjugate gradient** method.\n",
    "\n",
    "We have for $A = A^* > 0$\n",
    "\n",
    "$$A Q_j = Q_j T_j + T_{j, j-1} q_j.$$\n",
    "\n",
    "The approximate solution of $Ax \\approx f$ with $x_j = Q_j \\widehat{x}_j$ can be found by solving a small system\n",
    "\n",
    "$$Q^*_j A Q_j \\widehat x_j = T_j \\widehat{x}_j = Q^*_j f .$$\n",
    "\n",
    "Since $f$ is the first Krylov subspace, then \n",
    "**Note!!!** (recall what the first column in $Q_j$ is)\n",
    "$$Q^*_j f  = \\Vert f \\Vert_2^2 e_0 = \\gamma e_0.$$\n",
    "\n",
    "We have a tridiagonal system of equations for $\\widehat x$:\n",
    "\n",
    "$$T_j \\widehat{x}_j = \\gamma e_0$$\n",
    "\n",
    "and $x_j = Q_j \\widehat{x}_j$.\n",
    "\n",
    "Since $A$ is positive definite, $T_j$ is also positive definite, and it allows an LU-decomposition\n",
    "\n",
    "$T_j = L_j U_j$, where $L_j$ is a bidiagonal matrix with ones on the diagonal, $U_j$ is a upper bidiagonal matrix.\n",
    "\n",
    "We need to define one subdiagonal in $L$ (with elements $c_1, \\ldots, c_{j-1}$), main diagonal of $U_j$ (with elements $d_0, \\ldots, d_{j-1}$ and superdiagonal of $U_j$ (with elements $b_1, \\ldots, b_{j-1}$.\n",
    "\n",
    "They have convenient recurrences:\n",
    "\n",
    "$$c_i = b_i/d_{i-1}, \\quad d_i = \\begin{cases} a_1, & \\mbox{if } i = 0, \\\\\n",
    "a_i - c_i b_i, & \\mbox{if } i > 0. \\end{cases}$$\n",
    "\n",
    "For the solution we have\n",
    "\n",
    "$$x_j = Q_j T^{-1}_j \\gamma e_0  = \\gamma Q_j (L_j U_j)^{-1} e_0  = \\gamma Q_j U^{-1}_j L^{-1}_j e_0.$$\n",
    "\n",
    "We introduce two new quantities:  \n",
    "\n",
    "$$P_j = Q_j U^{-1}_j, \\quad z_j = \\gamma L^{-1}_j e_0.$$\n",
    "\n",
    "Due to the recurrence relations, we have\n",
    "\n",
    "$$P_j = \\begin{bmatrix} P_{j-1} & p_j \\end{bmatrix}, $$\n",
    "\n",
    "and \n",
    "\n",
    "$$z_j = \\begin{bmatrix} z_{j-1} \\\\ \\xi_{j} \\end{bmatrix}.$$\n",
    "\n",
    "For $p_j$ and $\\xi_j$ we have short-term recurrence relations (due to bidiagonal structure)\n",
    "\n",
    "$$p_j = \\frac{1}{d_j}\\left(q_j - b_j p_{j-1} \\right), \\quad \\xi_j = -c_j \\xi_{j-1}.$$\n",
    "\n",
    "Thus, we arrive at short-term recurrence for $x_j$:\n",
    "\n",
    "$$x_j = P_j z_j = P_{j-1} z_{j-1} + \\xi_j p_j = x_{j-1} + \\xi_j p_j.$$\n",
    "\n",
    "and $q_j$ are found from the Lanczos relation (see slides above).\n",
    "\n",
    "This method for solving linear systems is called a **direct Lanczos method**. It is closely related to the conjugate gradient method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct Lanczos method\n",
    "\n",
    "We have the direct Lanczos method, where we store \n",
    "\n",
    "$$p_{j-1}, q_j, x_{j-1}$$ to get a new estimate of $x_j$.\n",
    "\n",
    "The main problem is with $q_j$: we have the three-term recurrence, but in the floating point arithmetic \n",
    "the orthogonality is can be lost, leading to numerical errors.\n",
    "\n",
    "Let us do some demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8460192815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg as spla\n",
    "import scipy\n",
    "from scipy.sparse import csc_matrix\n",
    "n = 128\n",
    "ex = np.ones(n);\n",
    "A = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
    "rhs = np.ones(n)\n",
    "\n",
    "nit = 64\n",
    "q1 = rhs/np.linalg.norm(rhs)\n",
    "q2 = A.dot(q1)\n",
    "q2 = q2 - np.dot(q2, q1)*q1\n",
    "q2 = q2/np.linalg.norm(q2)\n",
    "qall = [q1, q2]\n",
    "for i in range(nit):\n",
    "    qnew = A.dot(qall[-1])\n",
    "    qnew = qnew - np.dot(qnew, qall[-1])*qall[-1]\n",
    "    qnew = qnew/np.linalg.norm(qnew)\n",
    "    qnew = qnew - np.dot(qnew, qall[-2])*qall[-2]\n",
    "    qnew = qnew/np.linalg.norm(qnew)\n",
    "    qall.append(qnew)\n",
    "qall_mat = np.vstack(qall).T\n",
    "print(np.linalg.norm(qall_mat.T.dot(qall_mat) - np.eye(qall_mat.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conjugate gradient method\n",
    "\n",
    "Instead of $q_j$ (last vector in the modified Gram-Schmidt process), it is more convenient to work with the **residual**\n",
    "\n",
    "$$r_j = f - A x_j.$$\n",
    "\n",
    "The resulting recurrency has the form\n",
    "\n",
    "$x_j = x_{j-1} + \\alpha_{j-1} p_{j-1}$\n",
    "\n",
    "$r_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}$\n",
    "\n",
    "$p_j = r_j + \\beta_j p_{j-1}$.\n",
    "\n",
    "Hence the name conjugate gradient: to the gradient $r_j$ we add a **conjugate direction** $p_j$.\n",
    "\n",
    "We have **orthogonality** of residuals (check!):\n",
    "\n",
    "$$(r_i, r_j) = 0, \\quad i \\ne j$$\n",
    "\n",
    "and **A-orthogonality** of conjugate directions (check!):\n",
    "\n",
    "$$ (A p_i, p_j) = 0,$$\n",
    "\n",
    "which can be checked from the definition.\n",
    "\n",
    "The equations for $\\alpha_j$ and $\\beta_j$ can be now defined explicitly from these two properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CG final formulas\n",
    "\n",
    "We have $(r_{j}, r_{j-1}) = 0 = (r_{j-1} - \\alpha_{j-1} A r_{j-1}, r_{j-1})$, \n",
    "\n",
    "thus\n",
    "\n",
    "$$\\alpha_{j-1} = \\frac{(r_{j-1}, r_{j-1})}{(A r_{j-1}, r_{j-1})}.$$\n",
    "\n",
    "In the similar way, we have\n",
    "\n",
    "$$\\beta_{j-1} = \\frac{(r_j, r_j)}{(r_{j-1}, r_{j-1})}.$$\n",
    "\n",
    "Recall that\n",
    "\n",
    "$x_j = x_{j-1} + \\alpha_{j-1} p_{j-1}$\n",
    "\n",
    "$r_j = r_{j-1} - \\alpha_{j-1}  A p_{j-1}$\n",
    "\n",
    "$p_j = r_j + \\beta_j p_{j-1}$.\n",
    "\n",
    "Only one matrix-by-vector product per iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some history\n",
    "More details here: https://www.siam.org/meetings/la09/talks/oleary.pdf\n",
    "\n",
    "When Hestenes worked on conjugate bases in 1936, he was advised by a\n",
    "Harvard professor that it was too obvious for publication\n",
    "- CG doesn’t work on slide rules.\n",
    "- CG has little advantage over Gauss elimination for computation with\n",
    "calculators.\n",
    "- CG is not well suited for a room of human computers – too much data\n",
    "exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of the CG method\n",
    "\n",
    "We need to store 3 vectors.\n",
    "\n",
    "Since it generates $A$-orthogonal sequence $p_1, \\ldots, p_N$, aften $n$ steps it should stop (i.e., $p_{N+1} = 0$.) \n",
    "\n",
    "In practice it does not have this property in finite precision, thus after its invention in 1952 by Hestens and Stiefel it was labeled **unstable**.\n",
    "\n",
    "In fact, it is a brilliant iterative method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A-optimality\n",
    "\n",
    "\n",
    "Energy functional can be written as \n",
    "\n",
    "$$(Ax, x) - 2(f, x) = (A (x - x_*), (x - x_*)) - (Ax _*, x_*),$$\n",
    "\n",
    "where $A x_* = f$. Up to a constant factor,\n",
    "\n",
    "$$ (A(x - x_*), (x -x_*)) = \\Vert x - x_* \\Vert^2_A$$\n",
    "\n",
    "is the **A-norm** of the error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence\n",
    "The CG method compute $x_k$ that minimizes the energy functional over the Krylov subspace, i.e. $x_k = p(A)f$, where $p$ is a polynomial of degree $k+1$, so\n",
    "\n",
    "$$\\Vert x_k - x_* \\Vert_A  =  \\inf\\limits_{p} \\Vert \\left(p(A) - A^{-1}\\right) f \\Vert_A. $$\n",
    "\n",
    "Using eigendecomposition of $A$ we have\n",
    "\n",
    "$$A = U \\Lambda U^*, \\quad  g = U^* f,$$ and\n",
    "\n",
    "$\\Vert x - x_* \\Vert^2_A = \\displaystyle{\\inf_p} \\Vert \\left(p(\\Lambda) - \\Lambda^{-1}\\right) g \\Vert_\\Lambda^2 = \n",
    "\\displaystyle{\\sum_{i=1}^n} \\frac{(\\lambda_i p(\\lambda_i) - 1)^2 g^2_i}{\\lambda_i} = \\displaystyle{\\inf_{q, q(0) = 1}} \\displaystyle{\\sum_{i=1}^n} \\frac{q(\\lambda_i)^2 g^2_i}{\\lambda_i}\n",
    "$\n",
    "\n",
    "Selection of the optimal $q$ depends on the eigenvalue distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Absolute and relative error\n",
    "\n",
    "We have\n",
    "$$\\Vert x - x_* \\Vert^2_A \\leq \\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} \\inf_{q, q(0)=1} \\max_{j} q({\\lambda_j})^2$$\n",
    "\n",
    "The first term is just $$\\sum_{i=1}^n \\frac{g^2_i}{\\lambda_i} = (A^{-1} f, f) = \\Vert x_* \\Vert^2_A.$$\n",
    "\n",
    "And we have relative error bound\n",
    "\n",
    "$$\\frac{\\Vert x - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})|,$$\n",
    "\n",
    "so if matrix has only 2 different eigenvalues, then there exists a polynomial of degree 2 such that $q({\\lambda_1}) =q({\\lambda_2})=0$, so in this case CG converges in 2 iterations.\n",
    "\n",
    "If eigenvalues are clustered and there are $l$ outliers, then after first $\\mathcal{O}(l)$ iterations CG will converge as if there are no outliers (and hence the effective condition number is smaller). <br>\n",
    "The intuition behind this fact is that after $\\mathcal{O}(l)$ iterations the polynomial has degree more than $l$ and thus is able to zero $l$ outliers.\n",
    "\n",
    "Let us find another useful upper-bound estimate of convergence. \n",
    "Since\n",
    "\n",
    "$$\n",
    "\\inf_{q, q(0)=1} \\max_{j} |q({\\lambda_j})| \\leq \\inf_{q, q(0)=1} \\max_{\\lambda\\in[\\lambda_\\min,\\lambda_\\max]} |q({\\lambda})|\n",
    "$$\n",
    "\n",
    "The last term is just the same as for the Chebyshev acceleration, thus the same \n",
    "upper convergence bound holds:\n",
    "\n",
    "$$\\frac{\\Vert x_k - x_* \\Vert_A }{\\Vert x_* \\Vert_A} \\leq \\gamma \\left( \\frac{\\sqrt{\\mathrm{cond}(A)}-1}{\\sqrt{\\mathrm{cond}(A)}+1}\\right)^k.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finite termination & clusters\n",
    "\n",
    "1. If $A$ has $m$ eigenvalues, CG converges in $m$ iterations.\n",
    "2. If $A$ has $m$ \"clusters\" of eigenvalues, CG converges cluster-by-cluster.\n",
    "\n",
    "As a result, better convergence than Chebyshev acceleration, but slightly higher cost per iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "CG is the method of choice for symmetric positive definite systems:\n",
    "\n",
    "1. $\\mathcal{O}(n)$ memory\n",
    "2. Square root of condition number in the estimates\n",
    "3. Automatic ignoring of the outliers/clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-symmetric systems and the generalized minimal residual method (GMRES)\n",
    "\n",
    "Before we discussed symmetric positive definite systems. What happens if $A$ is non-symmetric?\n",
    "\n",
    "We can still orthogonalize the Krylov subspace using Arnoldi process, and get\n",
    "\n",
    "$$A Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1}.$$\n",
    "\n",
    "Let us rewrite the latter expression as\n",
    "\n",
    "$$ A Q_j = Q_j H_j + h_{j,j-1}q_j e^{\\top}_{j-1} = Q_{j+1} \\widetilde H_j, \\quad \\widetilde H_j = \n",
    "\\begin{bmatrix} h_{0,0} & h_{0,1} & \\dots & h_{0,j-2} & h_{0,j-1} \\\\ h_{1,0} & h_{1,1} & \\dots & h_{1,j-2} & h_{1,j-1} \\\\ 0& h_{2,2} &  \\dots & h_{2,j-2} & h_{2,j-1} \\\\\n",
    "0& 0 & \\ddots & \\vdots & \\vdots  \\\\\n",
    "0& 0 &  & h_{j,j-1} & h_{j-1,j-1} \\\\ 0& 0 & \\dots & 0 & h_{j,j-1}\\end{bmatrix}$$\n",
    "\n",
    "Then, if we need to minimize the residual over the Krylov subspace, we have\n",
    "\n",
    "$$x_j = Q_j \\widehat{x_j} $$\n",
    "\n",
    "and $x_j$ has to be selected as \n",
    "\n",
    "$$ \\Vert A Q_j \\widehat{x_j} - f \\Vert_2 \\rightarrow \\min.$$\n",
    "\n",
    "Using the Arnoldi recursion, we have\n",
    "\n",
    "$$ \\Vert Q_{j+1} \\widetilde H_j \\widehat{x_j} -  f \\Vert_2 \\rightarrow \\min.$$\n",
    "\n",
    "Using the orthogonal invariance under multiplication by unitary matrix, we get\n",
    "\n",
    "$$ \\Vert \\widetilde H_j \\widehat{x_j} - \\gamma e_0 \\Vert_2 \\rightarrow \\min,$$\n",
    "\n",
    "where we have used that $Q^*_{j+1} f = \\gamma e_0.$\n",
    "\n",
    "This is just a linear least squares with $(j+1)$ equations and $j$ unknowns.\n",
    "\n",
    "The matrix is also upper Hesseberg, thus its QR factorization can be computed in a very cheap way.\n",
    "\n",
    "This allows the computation of $\\widehat{x}_j$. This method is called **GMRES** (generalized minimal residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of the GMRES\n",
    "\n",
    "- Minimizes the residual directly\n",
    "- No normal equations\n",
    "- Memory grows with the number of iterations as $\\mathcal{O}(j^2)$, so **restarts** typically implemented (just start GMRES from the new initial guess)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "\n",
    "- Iterative methods continued (BiCG, Minres, QMR), preconditioners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
